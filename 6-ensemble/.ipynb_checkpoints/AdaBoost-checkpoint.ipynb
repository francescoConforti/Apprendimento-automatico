{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import sklearn.datasets\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sklearn.datasets.make_hastie_10_2()\n",
    "X_train = X[0:8000,:]\n",
    "y_train = y[0:8000]\n",
    "X_test = X[8000:,:]\n",
    "y_test = y[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Implement the AdaBoost ensemble algorithm by completing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_error(predictions, labels):\n",
    "    errors = 0\n",
    "    for i in range(len(labels)):\n",
    "        if predictions[i] != labels[i]:\n",
    "            errors += 1\n",
    "    return errors / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, weakModel, T):\n",
    "        self.base_model = weakModel\n",
    "        self.num_iter = T\n",
    "        self.models = [weakModel] * T\n",
    "        self.alpha = [0] * T\n",
    "        self.K = 20\n",
    "            \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        w = [1/len(X)] * len(X)\n",
    "        for i in range(self.num_iter):\n",
    "            new_model = deepcopy(self.base_model)\n",
    "            new_model.fit(X, y, w)\n",
    "            predictions = new_model.predict(X)\n",
    "            self.models[i] = new_model\n",
    "            epsilon = 0\n",
    "            for j in range(len(X)):\n",
    "                if predictions[j] != y[j]:\n",
    "                    epsilon += w[j]\n",
    "            self.alpha[i] = 0.5 * np.log((1- epsilon) / epsilon)\n",
    "            for j in range(len(X)):\n",
    "                w[j] = w[j] * math.exp((-1) * self.alpha[i] * y[j] * predictions[j])\n",
    "            w = [float(wi)/sum(w) for wi in w] # normalize\n",
    "            if i % self.K == 0:\n",
    "                pass\n",
    "                #print(\"Training iterazione \" + str(i))\n",
    "                #print(\"Errore modello corrente: \" + str(classification_error(predictions, y)))\n",
    "                #print(\"Errore ensemble: \" + str(classification_error(self.predict(X, i), y)))\n",
    "        return self\n",
    "            \n",
    "\n",
    "    def predict(self, X, iterations=-1): # iterations is only for testing\n",
    "        if iterations == -1:\n",
    "            iterations = self.num_iter\n",
    "        res = [0] * len(X)\n",
    "        for i in range(iterations):\n",
    "            pred = self.models[i].predict(X)\n",
    "            weighted_pred = [p * self.alpha[i] for p in pred]\n",
    "            for j in range(len(X)):\n",
    "                res[j] += weighted_pred[j]\n",
    "        res = [np.sign(r) for r in res]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation you are free to assume:\n",
    "- that the problem is a binary classification problem with labels in $\\{-1, +1\\}$.\n",
    "- that the weakModel can fit a weighted sample set by means of the call `weakModel.fit(X,y,sample_weight=w)` where `w` is a vector of length $|y|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation on the dataset loaded above and using an SVC with a polynomial kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copio la classe svc personalizzata per motivi di compilazione\n",
    "class SVC_:\n",
    "    def __init__(self, kernel=\"rbf\", degree=\"3\"):\n",
    "        self.svc = SVC(kernel=kernel, degree=degree)\n",
    "\n",
    "    def fit(self, X,y,sample_weight=None):\n",
    "        if sample_weight is not None:\n",
    "            #sample_weight = sample_weight * len(X)\n",
    "            # la riga sopra dà errore, forse intende:\n",
    "            for w in sample_weight:\n",
    "                w *= len(X)\n",
    "\n",
    "        self.svc.fit(X,y,sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000644\n",
      "0.9999999999999968\n",
      "0.9999999999999972\n",
      "0.9999999999999956\n",
      "0.9999999999999974\n",
      "0.9999999999999976\n",
      "0.9999999999999969\n",
      "0.9999999999999984\n",
      "0.9999999999999386\n",
      "1.0000000000002691\n",
      "Adaboost:\n",
      "Classification error on train set: 0.498375\n",
      "Classification error on test set: 0.50775\n",
      "\n",
      "SVC:\n",
      "Classification error on train set: 0.349875\n",
      "Classification error on test set: 0.40275\n"
     ]
    }
   ],
   "source": [
    "#weakModel = SVC(kernel=\"poly\", degree=3)\n",
    "#adaboost = AdaBoost(weakModel, 100)\n",
    "#y_train_ = c.predict(X_train)\n",
    "#y_test_ = c.predict(X_test)\n",
    "\n",
    "# La parte sopra (da moodle) dà errore, provo a rifare\n",
    "\n",
    "weakModel = SVC_(kernel=\"poly\", degree=3)\n",
    "adaboost = AdaBoost(weakModel, 10).fit(X_train, y_train)\n",
    "pred_train = adaboost.predict(X_train)\n",
    "pred_test = adaboost.predict(X_test)\n",
    "\n",
    "print(\"Adaboost:\")\n",
    "print(\"Classification error on train set: \" + str(classification_error(pred_train, y_train)))\n",
    "print(\"Classification error on test set: \" + str(classification_error(pred_test, y_test)))\n",
    "\n",
    "print(\"\\nSVC:\")\n",
    "svc = SVC(kernel=\"poly\", degree=3).fit(X_train, y_train)\n",
    "pred_train = svc.predict(X_train)\n",
    "pred_test = svc.predict(X_test)\n",
    "print(\"Classification error on train set: \" + str(classification_error(pred_train, y_train)))\n",
    "print(\"Classification error on test set: \" + str(classification_error(pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and evaluate the AdaBoost performances as usual by calculating the classification error. \n",
    "\n",
    "**Note 1**:  \n",
    "since the labels are bound to be in ${+1, -1}$, the classification error can be easily computed as:\n",
    "$$\n",
    "   error(y,y') = \\frac{1}{2} - \\frac{y^T \\times y'}{2N},\n",
    "$$\n",
    "where $N$ is the total number of examples. The formula can be derived noticing that $y^T \\times y'$ calculates the number $N_c$ of examples correctly classified  minus the number $N_{\\bar c}$ of examples incorrectly classified. We have then $y^T \\times y' = N_c - N_{\\bar c}$ and by noticing that $N = N_c + N_{\\bar c}$:\n",
    "$$\n",
    "   N - y^T \\times y' = 2 N_{\\bar c} \\Rightarrow \\frac{N - y^T \\times y'}{2 N} = \\frac{N_{\\bar c}}{N} = error(y,y')\n",
    "$$\n",
    "\n",
    "**Note 2**:\n",
    "do not forget to deepcopy your base model before fitting it to the new data\n",
    "\n",
    "**Note 3**:\n",
    "The SVC model allows specifying weights, but it *does not* work well when weights are normalized (it works well when the weights are larger). The following class takes normalized weights and denormalize them before passing them to the SVC classifier:\n",
    "\n",
    "```python\n",
    "    class SVC_:\n",
    "        def __init__(self, kernel=\"rbf\", degree=\"3\"):\n",
    "            self.svc = SVC(kernel=kernel, degree=degree)\n",
    "\n",
    "        def fit(self, X,y,sample_weight=None):\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * len(X)\n",
    "\n",
    "            self.svc.fit(X,y,sample_weight=sample_weight)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.svc.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a weak learner to be used with the AdaBoost algorithm you just wrote. The weak learner that you will implement shall work as follows:\n",
    "\n",
    "- creates a random linear model $y(x) = \\mathbf{w} \\cdot \\mathbf{x} + t$ by generating the needed weight vector $\\mathbf{w}$ and $t$ at random; each weight shall be sampled from U(-1,1);\n",
    "- it evaluates the weighted loss $\\epsilon_t$ on the given dataset and flip the linear model if $\\epsilon_t > 0.5$\n",
    "- at prediction time it predicts +1 if $\\mathbf{x} \\cdot \\mathbf{w} > 0$ it predicts -1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearModel:\n",
    "    def loss(self, y, y_, w):\n",
    "        if w == None:\n",
    "            w = [1/len(y)] * len(y)\n",
    "        res = 0\n",
    "        for i in range(len(y)):\n",
    "            if y[i] != y_[i]:\n",
    "                res += w[i]\n",
    "        return res\n",
    "        \n",
    "    def fit(self,X,y,sample_weight=None):\n",
    "        self.flip = False\n",
    "        self.w = np.random.uniform(-1, 1, len(X[0]))\n",
    "        self.t = np.random.uniform(-1, 1)\n",
    "        weighted_loss = self.loss(y, self.predict(X), sample_weight)\n",
    "        if weighted_loss > 0.5:\n",
    "            self.flip = True\n",
    "        return self\n",
    "        \n",
    "    def predict(self,X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            s = 0\n",
    "            for feature in range(len(x)):\n",
    "                s += self.w[feature] * x[feature]\n",
    "            predictions.append(np.sign(s + self.t))\n",
    "        if self.flip:\n",
    "            predictions = [-p for p in predictions]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn an AdaBoost model using the RandomLinearModel weak learner printing every $K$ iterations the weighted error and the current error of the ensemble (you are free to choose $K$ so to make your output just frequent enough to let you know what is happening but without flooding the console with messages). Evaluate the training and test error of the final ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterazione 0\n",
      "Errore modello corrente: 0.479\n",
      "Errore ensemble: 1.0\n",
      "Training iterazione 20\n",
      "Errore modello corrente: 0.47425\n",
      "Errore ensemble: 0.462\n",
      "Training iterazione 40\n",
      "Errore modello corrente: 0.491125\n",
      "Errore ensemble: 0.4695\n",
      "Training iterazione 60\n",
      "Errore modello corrente: 0.48225\n",
      "Errore ensemble: 0.466875\n",
      "Training iterazione 80\n",
      "Errore modello corrente: 0.4825\n",
      "Errore ensemble: 0.468625\n",
      "Training iterazione 100\n",
      "Errore modello corrente: 0.498625\n",
      "Errore ensemble: 0.46575\n",
      "Training iterazione 120\n",
      "Errore modello corrente: 0.50725\n",
      "Errore ensemble: 0.466\n",
      "Training iterazione 140\n",
      "Errore modello corrente: 0.510875\n",
      "Errore ensemble: 0.466625\n",
      "Training iterazione 160\n",
      "Errore modello corrente: 0.472375\n",
      "Errore ensemble: 0.459625\n",
      "Training iterazione 180\n",
      "Errore modello corrente: 0.469375\n",
      "Errore ensemble: 0.453875\n",
      "Training iterazione 200\n",
      "Errore modello corrente: 0.481875\n",
      "Errore ensemble: 0.460125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-19e80e302db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomLinearModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_train_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-dad7c7dd6b9e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training iterazione \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-dad7c7dd6b9e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training iterazione \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rs = RandomLinearModel()\n",
    "a = AdaBoost(rs,10000)\n",
    "a.fit(X_train,y_train)\n",
    "\n",
    "y_train_ = a.predict(X_train)\n",
    "y_test_ = a.predict(X_test)\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "rs_train = rs.predict(X_train)\n",
    "rs_test = rs.predict(X_test)\n",
    "\n",
    "err_train = classification_error(y_train_, y_train)\n",
    "err_test_ = classification_error(y_test_, y_test)\n",
    "\n",
    "err_rs_train = classification_error(rs_train, y_train)\n",
    "err_rs_test_ = classification_error(rs_test, y_test)\n",
    "\n",
    "print(\"Error on training set adaboost: \" + str(err_train))\n",
    "print(\"Error on test set adaboost: \" + str(err_test_))\n",
    "print(\"Error on training set random linear model: \" + str(err_rs_train))\n",
    "print(\"Error on test set random linear model: \" + str(err_rs_test_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write few paragraphs about what you think about the experiment and about the results you obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
